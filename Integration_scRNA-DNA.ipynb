{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37e4a52a-ed0b-4fd9-9864-91b95d103005",
   "metadata": {},
   "source": [
    "# Genome-Transcriptome Integration using SIMBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e230bfc-0765-478e-a45b-0c82bc044d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import simba as si\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "print(si.__version__)\n",
    "print(sns.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c83ca9-fdca-4a5c-adfb-71d8a9613468",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.settings.set_figure_params(dpi=80,\n",
    "                              style='white',\n",
    "                              fig_size=[5,5],\n",
    "                              rc={'image.cmap': 'viridis'})\n",
    "\n",
    "# make plots prettier\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "set_matplotlib_formats('retina')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752a5611-00be-4cde-bce9-b2b60dff38f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = \"07-03-2024\" \n",
    "workdir = f'/mnt/d/JorritvU/SIMBA/full_integration/{project_name}/unfiltered'\n",
    "si.settings.set_workdir(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fa0a51-b15f-42f6-875c-0f8c7811fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Annotation files with Omic-specific cluster labels\n",
    "seurat_clusters = \"/mnt/d/JorritvU/Tripolar/scRNA-seq/s143/processed/Seurat/s143_Seuratdata.csv\"\n",
    "chaotic_sample_names_file = \"/mnt/d/JorritvU/Tripolar/scDNA-seq/chaotic_cnv_patterns.lst\" \n",
    "\n",
    "# File with barcode - well combination\n",
    "well_barcode_file = \"/mnt/d/JorritvU/Tripolar/scRNA-seq/SORTseq_cellbarcodes.tsv\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337b49e9-602a-4547-9e74-351cb22f5465",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the RNA plates\n",
    "\n",
    "adata_s143 = si.read_h5ad('/mnt/d/JorritvU/Tripolar/scRNA-seq/s143/old/SNV/s143.germline.updated.h5ad')\n",
    "adata_s145 = si.read_h5ad('/mnt/d/JorritvU/Tripolar/scRNA-seq/s145/old/SNV/s145.germline.updated.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e73ed5-f043-46e9-8a29-e52780528615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the DNA plates\n",
    "\n",
    "adata_chi006 = si.read_h5ad('/mnt/d/JorritvU/Tripolar/scDNA-seq/CHI-006/processed/SNV/CHI-006.germline_v2.h5ad')\n",
    "adata_chi007 = si.read_h5ad('/mnt/d/JorritvU/Tripolar/scDNA-seq/CHI-007/processed/SNV/CHI-007.germline_v2.h5ad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df29cd81-de81-4305-8605-3db9094f3422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attach Seurat clusters to RNA cells\n",
    "import re\n",
    "\n",
    "def create_well_annotations(seurat_clusters_mapping, well_barcode_file, adata):\n",
    "    \"\"\"Create a dictionary with well annotations from the well barcode match file.\"\"\"\n",
    "    well_annotations = {}\n",
    "    # Find the barcode - well match\n",
    "    with open(well_barcode_file, 'r') as file:\n",
    "        for line in file:\n",
    "            well,_ , barcode = line.strip().split('\\t')\n",
    "            well_annotations[barcode] = well\n",
    "\n",
    "    barcode_df = pd.DataFrame(list(well_annotations.items()), columns=['Barcode', 'Well'])\n",
    "\n",
    "    # Find the Barcode - SeuratCluster match\n",
    "    seurat_clusters_mapping['barcode_second_half'] = seurat_clusters_mapping['barcode'].str.split('_', expand=False).str[1]\n",
    "\n",
    "    seurat_clusters_mapping = pd.DataFrame({\n",
    "        'Barcode': seurat_clusters_mapping['barcode_second_half'],\n",
    "        'type_scluster': seurat_clusters_mapping['type_scluster']\n",
    "    })\n",
    "\n",
    "    # Combine the dataframes\n",
    "    combined_df = pd.merge(seurat_clusters_mapping, barcode_df, on='Barcode')\n",
    "    print(combined_df)\n",
    "    # Find wells in the name of the sample (e.g. s145_bi_F18_Bipolar -> F18) \n",
    "    #   And then attach to adata\n",
    "    pattern = r'_([AGCT]{8})_'\n",
    "    barcode_info = [re.search(pattern, name).group(1) if re.search(pattern, name) else None for name in adata.obs_names]\n",
    "    \n",
    "    adata_obs_df = pd.DataFrame({'obs_names': adata.obs_names, 'Barcode': barcode_info})\n",
    "    \n",
    "    well_to_cluster = combined_df[['Barcode', 'type_scluster']].set_index('Barcode').to_dict()['type_scluster']\n",
    "    bc_map = adata_obs_df['Barcode'].map(well_to_cluster)\n",
    "    print(bc_map)\n",
    "    adata.obs['cluster'] = list(bc_map)\n",
    "    \n",
    "    return adata\n",
    "\n",
    "\n",
    "seurat_clusters_mapping = pd.read_csv(seurat_clusters, header=0)\n",
    "adata_s143 = create_well_annotations(seurat_clusters_mapping, well_barcode_file, adata_s143)\n",
    "\n",
    "adata_s145 = create_well_annotations(seurat_clusters_mapping, well_barcode_file, adata_s145)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07307f4f-53c7-4364-9392-f5039a98690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the list of sample names with the chaotic cnv pattern\n",
    "\n",
    "with open(chaotic_sample_names_file) as fi:\n",
    "    sample_names = [line.strip().split('.')[0] for line in fi.readlines()]\n",
    "\n",
    "sample_names = {name.replace('âˆ’', '-') for name in sample_names}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee0900b-b8ae-443f-8e0d-0481a91c0ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the CNV profile type (chaotic or normal) to the cells.\n",
    "\n",
    "# Check if each obs_name is in the set of sample names and assign 1 or 0 accordingly\n",
    "adata_chi006.obs['cluster'] = [\"chaotic_cnv\" if name in sample_names else \"normal_cnv\" for name in adata_chi006.obs_names]\n",
    "\n",
    "# If you need to ensure that 'cluster' is of a specific dtype (e.g., int), you can enforce it like this:\n",
    "adata_chi006.obs['cluster'] = adata_chi006.obs['cluster'].astype(str)\n",
    "\n",
    "# Now the same for CHI-007\n",
    "adata_chi007.obs['cluster'] = [\"chaotic_cnv\" if name in sample_names else \"normal_cnv\" for name in adata_chi007.obs_names]\n",
    "adata_chi007.obs['cluster'] = adata_chi007.obs['cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dda83f5-8244-4474-8930-74ff60a90397",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_chi007.obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d97a92-1bf0-4a32-81bf-e5d7a47698cf",
   "metadata": {},
   "source": [
    "Now we filter on Allele Frequency (AF). <br/>\n",
    "For now arbitrary number (0.1). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ae72e9-586c-4d94-a793-aa4290552810",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_s143.var['pass'] = adata_s143.var['AF'] > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c58587-8c46-493a-8bb2-d18d74099dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_s145.var['pass'] = adata_s145.var['AF'] > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fce10d-7c05-4a71-a77c-9cbcecb7504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_chi006.var['pass'] = adata_chi006.var['AF'] > 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345b50a0-c9a9-40bb-899c-9133143ca5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_chi007.var['pass'] = adata_chi007.var['AF'] > 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4e3432-8980-4e5d-b943-323483b37381",
   "metadata": {},
   "source": [
    "## Merge the RNA runs into 1, and merge the DNA runs into 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36da5f74-796c-47d1-bfc9-40c5b0fa8de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anndata as ad\n",
    "\n",
    "def merge_datasets(adata1, adata2):\n",
    "    common_vars = list(set(adata1.var_names).intersection(set(adata2.var_names)))\n",
    "    print(f\"Number of intersecting SNVs: {len(common_vars)}\")\n",
    "    adata1 = adata1[:, common_vars]\n",
    "    adata2 = adata2[:, common_vars]\n",
    "    adata = ad.concat([adata1, adata2], merge='first', join='inner')\n",
    "    return adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1e9ff1-d6bb-4d97-87c8-dc696ad93120",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adata_dna = merge_datasets(adata_chi006, adata_chi007)  \n",
    "print(adata_dna)\n",
    "adata_rna = merge_datasets(adata_s143, adata_s145)  \n",
    "print(adata_rna)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a0414-e5af-4c23-ac32-6e16429161c6",
   "metadata": {},
   "source": [
    "# DNA and RNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ee13a4-58e7-4d03-91de-5f2acf7701bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Filter the NaN samples and based on AF > 0.05.\n",
    "If other sample types should be excluded, change code here.\n",
    "\"\"\"\n",
    "\n",
    "data = {'rna': adata_rna, 'dna': adata_dna}\n",
    "\n",
    "for k in ['rna', 'dna']:\n",
    "    data[f\"{k}_filtered\"] = data[k][~data[k].obs_names.str.contains('nan|Control', na=False), data[k].var['AF'] > 0.05].copy()\n",
    "    print(f\"{k}_filtered: {data[f'{k}_filtered'].shape}\")\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b32a170-954c-4cce-b651-694f0ee237e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_datasets = [d for d in data.keys() if 'filtered' in d]\n",
    "filtered_datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e92df6-ecc9-4b09-8b22-91865a97e7fb",
   "metadata": {},
   "source": [
    "## SIMBA\n",
    "Infer edges between cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2442537d-70b4-4d00-b836-8642a1d334f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Infer edges between all possible combinations of 'filtered_datasets'.\n",
    "In this case only 1 combination:\n",
    "rna_filtered - dna_filtered\n",
    "\"\"\"\n",
    "\n",
    "import itertools\n",
    "# Dictionary to store the CC_embeddings\n",
    "CC_embeddings = {}\n",
    "\n",
    "# Iterate over all pairs of datasets\n",
    "for dataset1, dataset2 in itertools.combinations(filtered_datasets, 2):\n",
    "    print(f\"Inferring edges between 1) {dataset1}, and 2) {dataset2}\")\n",
    "\n",
    "    print(data[dataset1].var_names)\n",
    "    print(data[dataset2].var_names)\n",
    "    \n",
    "    # Call si.tl.infer_edges for each combination\n",
    "    CC_embeddings[f'CC-{dataset1}-{dataset2}'] = si.tl.infer_edges(data[dataset1], data[dataset2], feature='pass', n_components=10, k=10)  # n_components & k need to be optimized\n",
    "\n",
    "CC_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40231b8b-ac20-4aa9-b943-22bacf04a5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Prune the DNA so that only the RNA SNVs are being kept to reduce complexity.\n",
    "Then create a new dictionary that includes the filtered datasets + the cell-cell embedding.\n",
    "'''\n",
    "\n",
    "all_datasets = {}\n",
    "\n",
    "# Overlap RNA-DNA, and prune on common features -> DNA is too big\n",
    "common_vars = set(data[filtered_datasets[1]].var_names).intersection(set(data[filtered_datasets[0]].var_names))\n",
    "\n",
    "# Filtered datasets 0 = RNA\n",
    "data[filtered_datasets[0]] = data[filtered_datasets[0]][:, list(common_vars)]\n",
    "\n",
    "# Filtered datasets 1 = DNA\n",
    "data[filtered_datasets[1]] = data[filtered_datasets[1]][:, list(common_vars)]\n",
    "\n",
    "\n",
    "for k in filtered_datasets:\n",
    "    print(k)\n",
    "    all_datasets[k] = data[k].copy()\n",
    "\n",
    "# Adding CC_embeddings as separate items in the list\n",
    "for key, value in CC_embeddings.items():\n",
    "    all_datasets[key] = value\n",
    "\n",
    "all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ac1761-1525-4077-bc3c-f468adfe9876",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ensure good naming within datasets.\n",
    "'''\n",
    "\n",
    "for dataset_name, dataset in all_datasets.items():\n",
    "    print('Checking', dataset_name)\n",
    "    assert dataset.var_names.is_unique, f\"var_names in {dataset_name} are not unique.\"\n",
    "    assert dataset.obs_names.is_unique, f\"obs_names in {dataset_name} are not unique.\"\n",
    "    assert dataset.var_names.notnull().all(), f\"var_names in {dataset_name} contain null values.\"\n",
    "    assert dataset.obs_names.notnull().all(), f\"obs_names in {dataset_name} contain null values.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03780fc-75f9-4b9a-b6fb-de8066da6ace",
   "metadata": {},
   "source": [
    "## Generate the graph and train the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f854f224-84bc-4182-b0ee-c673cee4ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.tl.gen_graph(list_adata=[x.copy() for x in all_datasets.values()], copy=False, layer=None, dirname=\"graph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007ab6cc-f09e-46c6-ac0c-b8e53159b1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.settings.pbg_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148322fd-ef84-46c7-8277-a6fc53e6776d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dict_config = si.settings.pbg_params.copy()\n",
    "dict_config['workers'] = 12\n",
    "\n",
    "# MKL_THREADING_LAYER is by default INTEL\n",
    "# By default numpy is trying to use Intel's implementation of OpenMP, while PyTorch is linked with GNU, which seem to trigger this error message\n",
    "# To avoid this, we set MKL_THREADING_LAYER to GNU, and this is an environment variable\n",
    "os.environ[\"MKL_THREADING_LAYER\"]=\"GNU\"\n",
    "\n",
    "## start training\n",
    "si.tl.pbg_train(pbg_params = dict_config, auto_wd=True, save_wd=True, output='model', use_edge_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c6939b-cc93-43ba-8595-2fb28c3e0c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.pl.pbg_metrics(fig_ncol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3278aeca-b8f5-4864-b1eb-76f71f8a09e2",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce78912-43e2-4d61-a232-2a1a2581ba38",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extract the embedding and assign correct names and variables.\n",
    "'''\n",
    "\n",
    "dict_adata = si.read_embedding()\n",
    "adata_C = dict_adata['E0']  # embeddings of cells from RNA\n",
    "adata_C2 = dict_adata['E2']  # embeddings for cells from DNA\n",
    "adata_S = dict_adata['E1']  # embeddings for SNVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08bd876-87a8-47d2-b486-f1b4f94017d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['rna_filtered'].obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011e681-b575-4321-a201-3dc696a26ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_C.obs['phenotype'] = data['rna_filtered'][adata_C.obs_names,:].obs['Phenotype'].copy()\n",
    "adata_C.obs['cluster'] = data['rna_filtered'][adata_C.obs_names,:].obs['cluster'].copy()\n",
    "adata_C.obs['batch'] = data['rna_filtered'][adata_C.obs_names,:].obs['Batch'].copy()\n",
    "si.tl.umap(adata_C,n_neighbors=15,n_components=2)\n",
    "\n",
    "adata_C2.obs['phenotype'] = data['dna_filtered'][adata_C2.obs_names,:].obs['Phenotype'].copy()\n",
    "adata_C2.obs['cluster'] = data['dna_filtered'][adata_C2.obs_names,:].obs['cluster'].copy()\n",
    "adata_C2.obs['batch'] = data['dna_filtered'][adata_C2.obs_names,:].obs['Batch'].copy()\n",
    "si.tl.umap(adata_C2,n_neighbors=15,n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952fdd00-a898-4220-b06f-b3837ff90e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the RNA as the reference, and DNA as the query.\n",
    "'''\n",
    "\n",
    "adata_all = si.tl.embed(adata_ref=adata_C,list_adata_query=[adata_C2])\n",
    "\n",
    "## add annotations of two batches\n",
    "adata_all.obs['entity_group'] = \"\"\n",
    "adata_all.obs.loc[adata_C.obs_names, 'entity_group'] = \"rna\"\n",
    "adata_all.obs.loc[adata_C2.obs_names, 'entity_group'] = \"dna\"\n",
    "\n",
    "# Add entity group to the SNVs if the SNVs embeddings is included in the query.\n",
    "# adata_all.obs.loc[adata_S.obs_names, 'entity_group'] = \"SNV\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0469c54c-1d23-499e-bd12-80e66f410a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "clusters = list(adata_all.obs['cluster'])\n",
    "for i, c in enumerate(clusters):\n",
    "    if c is not None and isinstance(c, float) and math.isnan(c):\n",
    "        clusters[i] = 'Unclustered'\n",
    "\n",
    "adata_all.obs['cluster'] = clusters\n",
    "adata_all.obs['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342a2627-1f9e-4635-9b71-647aaa3331df",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_all.X[1,:]  ## This is what 1 embedding looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2fbb94-a5b5-442a-80aa-f555fefefc1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "si.tl.umap(adata_all,n_neighbors=20,n_components=2)  # n_components & n_neighbors need to be optimized\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "UMAPs to save to file\n",
    "    \n",
    "\"\"\"\n",
    "si.pl.umap(adata_C,\n",
    "               color=['phenotype', 'batch'],\n",
    "               fig_size=(5.5, 4),\n",
    "               drawing_order='random',\n",
    "               save_fig=True, fig_path=f\"{workdir}\", fig_name=f\"UMAP_RNA.pdf\")\n",
    "    \n",
    "si.pl.umap(adata_C2,\n",
    "               color=['phenotype', 'batch'],\n",
    "               fig_size=(5.5, 4),\n",
    "               drawing_order='random',\n",
    "               save_fig=True, fig_path=f\"{workdir}\", fig_name=f\"UMAP_DNA.pdf\")\n",
    "    \n",
    "si.pl.umap(adata_all,color=['entity_group', 'batch','phenotype','cluster'],\n",
    "               drawing_order='random',\n",
    "               fig_size=(5.5,4),\n",
    "               save_fig=True, fig_path=f\"{workdir}\", fig_name=f\"UMAP_integrated.pdf\")\n",
    "    \n",
    "\n",
    "\n",
    "si.pl.umap(adata_C,\n",
    "           color=['phenotype', 'batch'],\n",
    "           fig_size=(5.5, 4),\n",
    "           drawing_order='random')\n",
    "\n",
    "si.pl.umap(adata_C2,\n",
    "           color=['phenotype', 'batch'],\n",
    "           fig_size=(5.5, 4),\n",
    "           drawing_order='random')\n",
    "\n",
    "si.pl.umap(adata_all,color=['entity_group', 'batch','phenotype','cluster'],\n",
    "           drawing_order='random',\n",
    "           fig_size=(5.5,4),\n",
    "           save_fig=False, fig_path=f\"{workdir}\", fig_name=\"UMAP_noControl.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7423763-298d-4b7c-981d-c905a6ba1dda",
   "metadata": {},
   "source": [
    "# SIMBA integration with filtered SNVs based on proportions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba1b70-7f53-4145-afdf-f558390e1b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "workdir = f'/mnt/d/JorritvU/SIMBA/full_integration/{project_name}/filtered'\n",
    "si.settings.set_workdir(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0306f890-daae-4a24-bf4b-51b44a3a556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function to add proportions per SNV\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def calc_props(adata):\n",
    "    X_dense = adata.X.toarray()\n",
    "    \n",
    "    # Count occurrences of each variant type (1, 2, 3) per cell\n",
    "    variant_counts = np.apply_along_axis(lambda x: np.bincount(x, minlength=4)[1:], axis=1, arr=X_dense)\n",
    "    \n",
    "    # Calculate proportions\n",
    "    variant_proportions = variant_counts / variant_counts.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Add proportions back to adata as layers or as part of obs (depending on your preference)\n",
    "    # Add variant proportions as separate columns in adata.obs\n",
    "    data[key].obs['variant_1_proportion_0/0'] = variant_proportions[:, 0]\n",
    "    data[key].obs['variant_2_proportion_0/1'] = variant_proportions[:, 1]\n",
    "    data[key].obs['variant_3_proportion_1/1'] = variant_proportions[:, 2]\n",
    "\n",
    "\n",
    "    snv_counts = np.zeros((adata.n_vars, 3), dtype=int)\n",
    "    \n",
    "    # Iterate over each variant type and count occurrences per SNV\n",
    "    for variant_type in range(1, 4):\n",
    "        snv_counts[:, variant_type-1] = np.sum(X_dense == variant_type, axis=0)\n",
    "\n",
    "    # Calculate the total counts per SNV to use for proportion calculation\n",
    "    total_snv_counts = snv_counts.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    # Calculate proportions of each variant type per SNV\n",
    "    snv_proportions = snv_counts / total_snv_counts\n",
    "\n",
    "    # Add SNV proportions to the .var DataFrame\n",
    "    adata.var['variant_1_proportion_0/0'] = snv_proportions[:, 0]\n",
    "    adata.var['variant_2_proportion_0/1'] = snv_proportions[:, 1]\n",
    "    adata.var['variant_3_proportion_1/1'] = snv_proportions[:, 2]\n",
    "\n",
    "    return adata\n",
    "\n",
    "\"\"\"\n",
    "Function to filter the data\n",
    "\"\"\"\n",
    "\n",
    "def compare_proportions(adata, adata1, var_name, t=0.01, window=0.2, debug=False):\n",
    "    print(\"\\nChecking\", var_name) if debug else print(\"\", end=\"\")\n",
    "    variants = ['variant_1_proportion_0/0', 'variant_2_proportion_0/1', 'variant_3_proportion_1/1']\n",
    "    p1 = [adata.var[k][var_name] for k in variants]\n",
    "    p2 = [adata1.var[k][var_name] for k in variants]\n",
    "    ratios = [(p2[i]/p1[i]) for i in range(len(variants))]\n",
    "\n",
    "    if debug:\n",
    "        print(f\"p1: {p1}\")\n",
    "        print(f\"p2: {p2}\")\n",
    "        print(f\"ratio: {ratios}\")\n",
    "    \n",
    "    if max(p1) >= 0.999 or max(p2) >= 0.999:\n",
    "        print(f\"BAD: {var_name} solely 1 variant\") if debug else print(\"\", end=\"\")\n",
    "        return \n",
    "        \n",
    "    for i, r in enumerate(ratios):\n",
    "        if r < 1-window or r > 1+window:\n",
    "            if p1[i] > t and p2[i] > t:\n",
    "                print(\"BAD RATIO\") if debug else print(\"\", end=\"\")\n",
    "                return \n",
    "\n",
    "    return {var_name: ratios}\n",
    "\n",
    "def strict_filtering(adata, adata1, var_name, t=0.01, window=0.05, debug=False):\n",
    "    print(\"\\nChecking\", var_name) if debug else print(\"\", end=\"\")\n",
    "    variants = ['variant_1_proportion_0/0', 'variant_2_proportion_0/1', 'variant_3_proportion_1/1']\n",
    "    p1 = [adata.var[k][var_name] for k in variants]\n",
    "    p2 = [adata1.var[k][var_name] for k in variants]\n",
    "    \n",
    "    for i in range(len(variants)):\n",
    "        rna_p = p1[i]\n",
    "        dna_p = p2[i]\n",
    "        if dna_p + window > rna_p and dna_p - window < rna_p:\n",
    "            print(f\"{dna_p} is very close to being equal to {rna_p}\") if debug else print(\"\", end=\"\")\n",
    "        else:\n",
    "            return\n",
    "    \n",
    "    print(\"All three are good\") if debug else print(\"\", end=\"\")\n",
    "    \n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10640a8-e4a8-40d2-94ec-aa2d0655ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in filtered_datasets:\n",
    "    data[key] = calc_props(data[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1031713-7f77-4cdb-90ab-7fe36c0cc824",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "STRICT filtering, to make the profiles across modalities more similar\n",
    "\"\"\"\n",
    "\n",
    "# snvs = list(data['rna_filtered'].var_names)\n",
    "\n",
    "# len_before = len(snvs)\n",
    "# good_snvs = []\n",
    "\n",
    "# for s in snvs:\n",
    "#     keep = strict_filtering(data['rna_filtered'], data['dna_filtered'], s, window=0.03, debug=False)\n",
    "#     if keep:\n",
    "#         good_snvs.append(s)\n",
    "\n",
    "# len_after = len(good_snvs)\n",
    "\n",
    "# print(f\"No. SNVs before: {len_before}\")\n",
    "# print(f\"No. SNVs after: {len_after}\")\n",
    "# print(f\"Percentage thrown out: {100-round(len_after/len_before*100, 2)}%\")\n",
    "\n",
    "# ## Filter the datasets based on these SNVs\n",
    "\n",
    "# # Filtered datasets 0 = RNA\n",
    "# data[filtered_datasets[0]] = data[filtered_datasets[0]][:, list(good_snvs)]\n",
    "\n",
    "# # Filtered datasets 1 = DNA\n",
    "# data[filtered_datasets[1]] = data[filtered_datasets[1]][:, list(good_snvs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b083fc6-3ef9-4a0b-9428-7d66b6a44a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Lenient filtering with ratios and a bigger window\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "snvs = list(data['rna_filtered'].var_names)\n",
    "\n",
    "\n",
    "len_before = len(snvs)\n",
    "good_snvs = []\n",
    "\n",
    "for s in snvs:\n",
    "    keep = compare_proportions(data['rna_filtered'], data['dna_filtered'], s, window=0.3, t=0.1, debug=False)\n",
    "    if keep:\n",
    "        good_snvs.append(list(keep.keys())[0])\n",
    "\n",
    "len_after = len(good_snvs)\n",
    "\n",
    "print(f\"No. SNVs before: {len_before}\")\n",
    "print(f\"No. SNVs after: {len_after}\")\n",
    "print(f\"Percentage thrown out: {100-round(len_after/len_before*100, 2)}%\")\n",
    "\n",
    "## Filter the datasets based on these SNVs\n",
    "\n",
    "# Filtered datasets 0 = RNA\n",
    "data[filtered_datasets[0]] = data[filtered_datasets[0]][:, list(good_snvs)]\n",
    "\n",
    "# Filtered datasets 1 = DNA\n",
    "data[filtered_datasets[1]] = data[filtered_datasets[1]][:, list(good_snvs)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6531b349-b5bd-4754-a4c0-85e48922fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RNA\")\n",
    "data[filtered_datasets[0]].var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4488ca1f-4b56-4102-b8a3-86b8d74fcf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DNA\")\n",
    "data[filtered_datasets[1]].var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc3538-0bb8-40df-8699-368e4b5a5160",
   "metadata": {},
   "source": [
    "## SIMBA: infer edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95edf2f6-e4bb-4336-8d8c-88f1d335f6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Infer edges between all possible combinations of 'filtered_datasets'.\n",
    "In this case only 1 combination:\n",
    "rna_filtered - dna_filtered\n",
    "\"\"\"\n",
    "\n",
    "import itertools\n",
    "# Dictionary to store the CC_embeddings\n",
    "CC_embeddings = {}\n",
    "\n",
    "# Iterate over all pairs of datasets\n",
    "for dataset1, dataset2 in itertools.combinations(filtered_datasets, 2):\n",
    "    print(f\"Inferring edges between 1) {dataset1}, and 2) {dataset2}\")\n",
    "\n",
    "    print(data[dataset1].var_names)\n",
    "    print(data[dataset2].var_names)\n",
    "    \n",
    "    # Call si.tl.infer_edges for each combination\n",
    "    CC_embeddings[f'CC-{dataset1}-{dataset2}'] = si.tl.infer_edges(data[dataset1], data[dataset2], feature='pass', n_components=10, k=10)  # n_components & k need to be optimized\n",
    "\n",
    "CC_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de748908-063e-4e7f-9a40-7d043e60e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = {}\n",
    "\n",
    "for key in filtered_datasets:\n",
    "    all_datasets[key] = data[key]\n",
    "\n",
    "# Adding CC_embeddings as separate items in the list\n",
    "for key, value in CC_embeddings.items():\n",
    "    all_datasets[key] = value\n",
    "\n",
    "all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94b109-badb-4fde-a739-f5b57a6328f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Ensure good naming within datasets.\n",
    "'''\n",
    "\n",
    "for dataset_name, dataset in all_datasets.items():\n",
    "    print('Checking', dataset_name)\n",
    "    assert dataset.var_names.is_unique, f\"var_names in {dataset_name} are not unique.\"\n",
    "    assert dataset.obs_names.is_unique, f\"obs_names in {dataset_name} are not unique.\"\n",
    "    assert dataset.var_names.notnull().all(), f\"var_names in {dataset_name} contain null values.\"\n",
    "    assert dataset.obs_names.notnull().all(), f\"obs_names in {dataset_name} contain null values.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbece6d-efae-4e89-8be1-af155ae4d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447b4e8e-17e9-4f16-9b6e-6902c8e81105",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.tl.gen_graph(list_adata=[x.copy() for x in all_datasets.values()], copy=False, layer=None, dirname=\"graph2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0ed9a2-90e1-4278-ae8b-beba9b17e755",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_config = si.settings.pbg_params.copy()\n",
    "dict_config['workers'] = 12\n",
    "\n",
    "\n",
    "\n",
    "# MKL_THREADING_LAYER is by default INTEL\n",
    "# By default numpy is trying to use Intel's implementation of OpenMP, while PyTorch is linked with GNU, which seem to trigger this error message\n",
    "# To avoid this, we set MKL_THREADING_LAYER to GNU, and this is an environment variable\n",
    "os.environ[\"MKL_THREADING_LAYER\"]=\"GNU\"\n",
    "\n",
    "## start training\n",
    "si.tl.pbg_train(pbg_params = dict_config, auto_wd=True, save_wd=True, output='model2', use_edge_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b19023-2df5-4fe2-b7a0-a2708993bdf0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "si.pl.pbg_metrics(fig_ncol=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a89dff-8173-4985-a24f-ecab690d3dff",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47825b94-9cf0-4631-9a67-a091b3967b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Extract the embedding and assign correct names and variables.\n",
    "'''\n",
    "\n",
    "dict_adata = si.read_embedding()\n",
    "adata_C = dict_adata['E0']  # embeddings of cells from RNA\n",
    "adata_C2 = dict_adata['E2']  # embeddings for cells from DNA\n",
    "adata_S = dict_adata['E1']  # embeddings for SNVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97e8e1-2b82-45db-9bf4-64473e77ea64",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_C.obs['phenotype'] = data['rna_filtered'][adata_C.obs_names,:].obs['Phenotype'].copy()\n",
    "adata_C.obs['cluster'] = data['rna_filtered'][adata_C.obs_names,:].obs['cluster'].copy()\n",
    "adata_C.obs['batch'] = data['rna_filtered'][adata_C.obs_names,:].obs['Batch'].copy()\n",
    "si.tl.umap(adata_C,n_neighbors=15,n_components=2)\n",
    "\n",
    "adata_C2.obs['phenotype'] = data['dna_filtered'][adata_C2.obs_names,:].obs['Phenotype'].copy()\n",
    "adata_C2.obs['cluster'] = data['dna_filtered'][adata_C2.obs_names,:].obs['cluster'].copy()\n",
    "adata_C2.obs['batch'] = data['dna_filtered'][adata_C2.obs_names,:].obs['Batch'].copy()\n",
    "\n",
    "adata_S.obs['phenotype'] = \"SNV\"\n",
    "adata_S.obs['cluster'] = \"NA\"\n",
    "adata_S.obs['batch'] = \"SNV\"\n",
    "si.tl.umap(adata_C2,n_neighbors=15,n_components=2)\n",
    "si.tl.umap(adata_S,n_neighbors=15,n_components=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac013d-658a-43a9-966a-e3d0e5f8c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Use the RNA as the reference, and DNA as the query.\n",
    "'''\n",
    "import math\n",
    "\n",
    "adata_all = si.tl.embed(adata_ref=adata_C,list_adata_query=[adata_C2])\n",
    "\n",
    "## add annotations of two batches\n",
    "adata_all.obs['entity_group'] = \"\"\n",
    "adata_all.obs.loc[adata_C.obs_names, 'entity_group'] = \"rna\"\n",
    "adata_all.obs.loc[adata_C2.obs_names, 'entity_group'] = \"dna\"\n",
    "# adata_all.obs.loc[adata_S.obs_names, 'entity_group'] = \"SNV\"\n",
    "# adata_all.obs.loc[adata_S.obs_names, 'cluster'] = \"SNV\"\n",
    "# adata_all.obs.loc[adata_S.obs_names, 'phenotype'] = \"SNV\"\n",
    "# adata_all.obs.loc[adata_S.obs_names, 'batch'] = \"SNV\"\n",
    "\n",
    "clusters = list(adata_all.obs['cluster'])\n",
    "for i, c in enumerate(clusters):\n",
    "    if c is not None and isinstance(c, float) and math.isnan(c):\n",
    "        clusters[i] = 'Unclustered'\n",
    "\n",
    "adata_all.obs['cluster'] = clusters\n",
    "adata_all.obs['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8bd13a-a495-4613-8df2-9bf8c5c1da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(workdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7d15e8-0309-4615-abb1-57ee9512001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbor = [5, 10, 15, 20, 30, 50]\n",
    "\n",
    "for n in neighbor:\n",
    "    print(n)\n",
    "    n_neighbors = n\n",
    "    n_components = 2\n",
    "    \n",
    "    si.tl.umap(adata_all,n_neighbors=n_neighbors,n_components=n_components)  # n_components & n_neighbors need to be optimized\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    UMAPs to save to file\n",
    "    \n",
    "    \"\"\"\n",
    "    si.pl.umap(adata_C,\n",
    "               color=['phenotype', 'batch'],\n",
    "               fig_size=(5.5, 4),\n",
    "               alpha=0.7, \n",
    "               drawing_order='random',\n",
    "               save_fig=True, fig_path=f\"{workdir}/RNA\", fig_name=f\"UMAP_RNA_param-nNB{n_neighbors}-nC{n_components}.pdf\")\n",
    "    \n",
    "    si.pl.umap(adata_C2,\n",
    "               color=['phenotype', 'batch'],\n",
    "               fig_size=(5.5, 4),\n",
    "               alpha=0.7, \n",
    "               drawing_order='random',\n",
    "               save_fig=True, fig_path=f\"{workdir}/DNA\", fig_name=f\"UMAP_DNA_param-nNB{n_neighbors}-nC{n_components}.pdf\")\n",
    "\n",
    "    si.pl.umap(adata_S,\n",
    "               fig_size=(5.5, 4),\n",
    "               alpha=0.7, \n",
    "               drawing_order='random',\n",
    "               save_fig=True, fig_path=f\"{workdir}/SNV\", fig_name=f\"UMAP_SNVs_param-nNB{n_neighbors}-nC{n_components}.pdf\")\n",
    "    \n",
    "    si.pl.umap(adata_all,color=['entity_group', 'batch','phenotype','cluster'],\n",
    "               drawing_order='random',\n",
    "               alpha=0.7, \n",
    "               fig_size=(5.5,4),\n",
    "               save_fig=True, fig_path=f\"{workdir}/integrated\", fig_name=f\"UMAP_integrated_param-nNB{n_neighbors}-nC{n_components}.pdf\")\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "        UMAP to show in output cell\n",
    "    \"\"\"\n",
    "    \n",
    "    si.pl.umap(adata_C,\n",
    "               color=['phenotype', 'batch'],\n",
    "               fig_size=(5.5, 4),\n",
    "               alpha=0.7, \n",
    "               drawing_order='random')\n",
    "    \n",
    "    si.pl.umap(adata_C2,\n",
    "               color=['phenotype', 'batch'],\n",
    "               fig_size=(5.5, 4),\n",
    "               alpha=0.7, \n",
    "               drawing_order='random')\n",
    "\n",
    "    si.pl.umap(adata_S,\n",
    "               fig_size=(5.5, 4),\n",
    "               alpha=0.7, \n",
    "               drawing_order='random')\n",
    "    \n",
    "    si.pl.umap(adata_all,color=['entity_group', 'batch','phenotype','cluster'],\n",
    "               drawing_order='random',\n",
    "               alpha=0.7, \n",
    "               fig_size=(5.5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb25548-6406-4109-b221-059f2417ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_all = si.tl.embed(adata_ref=adata_S,list_adata_query=[adata_C, adata_C2])\n",
    "\n",
    "## add annotations of two batches\n",
    "adata_all.obs['entity_group'] = \"\"\n",
    "adata_all.obs.loc[adata_C.obs_names, 'entity_group'] = \"rna\"\n",
    "adata_all.obs.loc[adata_C2.obs_names, 'entity_group'] = \"dna\"\n",
    "adata_all.obs.loc[adata_S.obs_names, 'entity_group'] = \"SNV\"\n",
    "adata_all.obs.loc[adata_S.obs_names, 'cluster'] = \"SNV\"\n",
    "adata_all.obs.loc[adata_S.obs_names, 'phenotype'] = \"SNV\"\n",
    "adata_all.obs.loc[adata_S.obs_names, 'batch'] = \"SNV\"\n",
    "\n",
    "clusters = list(adata_all.obs['cluster'])\n",
    "for i, c in enumerate(clusters):\n",
    "    if c is not None and isinstance(c, float) and math.isnan(c):\n",
    "        clusters[i] = 'Unclustered'\n",
    "\n",
    "adata_all.obs['cluster'] = clusters\n",
    "adata_all.obs['cluster'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43194ea-997f-431d-83f4-451badb38449",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change CNV clusters to include phenotype\n",
    "\n",
    "selected = adata_all.obs.loc[(adata_all.obs[\"entity_group\"] == \"dna\"), ['cluster', 'phenotype']]\n",
    "# Assuming 'selected' is your DataFrame from the previous operation\n",
    "selected['cluster'] = selected.apply(lambda row: f\"{row['cluster']}_{row['phenotype'].lower()}\" if row['cluster'] == \"normal_cnv\" else row['cluster'], axis=1)\n",
    "adata_all.obs.loc[selected.index, 'cluster'] = selected['cluster']\n",
    "\n",
    "adata_all.obs.loc[:, ['cluster', 'phenotype']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2831c0-efdd-435f-8518-a767d0319e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "NNeighbors=15\n",
    "NComp=2\n",
    "\n",
    "si.tl.umap(adata_all,n_neighbors=NNeighbors,n_components=NComp)  \n",
    "si.pp.pca(adata_all, n_components=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc7c817-8b8a-46dc-99ff-164a564d3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_palette = {\n",
    "    'entity_group': {'dna': '#f46806', 'rna': '#47aa26', 'SNV': '#123456'},\n",
    "    'phenotype': {'Tripolar': '#f46806', 'Bipolar': '#47aa26', 'SNV': '#123456'},\n",
    "    'batch': {'s143': '#aa2647', 's145': '#2647aa', 'CHI-006': '#f46806', 'CHI-007': '#40e0d0', 'SNV': '#123456'},\n",
    "    'cluster': {'SNV': '#123456', 'Seurat_0': '#f46806', \"Seurat_1\": \"#47aa26\", \"Seurat_2\": \"#516cbb\", \"chaotic_cnv\": \"#e2068c\", \"normal_cnv_bipolar\": \"#aaaaaa\", \"normal_cnv_tripolar\": \"#10001d\", \"Unclustered\": \"#eccdb8\"}  # Example colors\n",
    "}\n",
    "ps = set(list(adata_all.obs['cluster']))\n",
    "ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e52bf72-0b0c-4aa8-8716-fc069496b44f",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.pl.umap(adata_all,color=['cluster', 'phenotype', 'entity_group'],\n",
    "               dict_palette=dict_palette,\n",
    "               drawing_order='random',\n",
    "               alpha=0.7, \n",
    "               fig_size=(5.5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0682a6e2-c988-422c-91e0-2e8c8f4e72ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.pl.umap(adata_all,color=['batch', 'cluster', 'phenotype'],\n",
    "               drawing_order='random',\n",
    "               dict_palette=dict_palette,\n",
    "               alpha=0.9, \n",
    "               fig_size=(7,7),\n",
    "          save_fig=True, fig_path=f\"{workdir}/SNV_projected\", fig_name=f\"umap_cluster_with_SNVs.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5904b5-12c2-4a19-9dd2-cffb0db1f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_all = adata_all[np.where(adata_all.obs[\"id_dataset\"] != \"ref\")[0]]\n",
    "adata_all = adata_all[np.where(adata_all.obs[\"cluster\"] != \"Unclustered\")[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9770e4c-f094-41c1-8dd5-61faf9779cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.pl.umap(adata_all,color=['entity_group', 'batch','phenotype','cluster'],\n",
    "               drawing_order='random',\n",
    "               dict_palette=dict_palette,\n",
    "               alpha=0.7, \n",
    "               fig_size=(5.5,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab88c05-615f-4e82-8bab-8d2689401792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b11216-7365-46d8-910a-0bd6f46adacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata_all_bp = adata_all[np.where(adata_all.obs[\"phenotype\"] == \"Bipolar\")[0]]\n",
    "adata_all_tp = adata_all[np.where(adata_all.obs[\"phenotype\"] == \"Tripolar\")[0]]\n",
    "adata_all_bp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789526ab-8200-4170-9cdb-b706235566b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "si.pl.umap(adata_all,color=['batch', 'cluster', 'phenotype'],\n",
    "               drawing_order='random',\n",
    "               dict_palette=dict_palette,\n",
    "               alpha=0.9, \n",
    "               fig_size=(7,7),\n",
    "          save_fig=True, fig_path=f\"{workdir}/SNV_projected\", fig_name=f\"umap_cluster_noSNVs.pdf\")\n",
    "# si.pl.umap(adata_all_tp,color=['cluster'],\n",
    "#                drawing_order='random',\n",
    "#                alpha=0.9, \n",
    "#                fig_size=(5,5),\n",
    "#           save_fig=True, fig_path=f\"{workdir}/integrated2\", fig_name=f\"umap_TPcluster.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609d469a-e163-471f-8f9b-a11e349cf079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.colors as mc\n",
    "import numpy as np\n",
    "\n",
    "def heatmap_similarity(adata, phenotype=True, cluster=True, layer=\"X_umap\", d=\"euclidean\", c=True, name=\"Heatmap\"):\n",
    "    # Calculate cosine similarity matrix\n",
    "    #similarity_matrix = cosine_similarity(adata.X)\n",
    "\n",
    "    name = f\"{name}_{'Clustered_' if c else ''}{d}.{layer}.pdf\"\n",
    "    \n",
    "    if layer == \"X\":\n",
    "        l = adata.X\n",
    "    else:\n",
    "        l = adata.obsm[layer]\n",
    "    if d == \"euclidean\":\n",
    "        similarity_matrix = euclidean_distances(l)\n",
    "    elif d == \"cosine\":\n",
    "        similarity_matrix = cosine_similarity(l)\n",
    "\n",
    "    print(adata.obs.sort_values(by='cluster', inplace=True))\n",
    "    \n",
    "    # Reset index of .obs to ensure alignment\n",
    "    obs_data = adata.obs.reset_index(drop=True)\n",
    "    \n",
    "    col_colors = []\n",
    "    legend_info = []\n",
    "    \n",
    "    \n",
    "    # Prepare phenotype colors\n",
    "    if phenotype:\n",
    "        phenotype_palette = sns.color_palette(\"hls\", len(obs_data['entity_group'].unique()))\n",
    "        phenotype_color_map = {phenotype: color for phenotype, color in zip(obs_data['entity_group'].unique(), phenotype_palette)}\n",
    "        phenotype_colors = [phenotype_color_map[phenotype] for phenotype in obs_data['entity_group']]\n",
    "        phenotype_hex_colors = {key: mc.to_hex(value) for key, value in phenotype_color_map.items()}\n",
    "        \n",
    "    else:\n",
    "        phenotype_colors = None\n",
    "\n",
    "    # Prepare cluster colors\n",
    "    if cluster:\n",
    "        cluster_palette = sns.color_palette(\"bright\", len(obs_data['cluster'].unique()))\n",
    "        cluster_color_map = {cluster: color for cluster, color in zip(obs_data['cluster'].unique(), cluster_palette)}\n",
    "        cluster_colors = [cluster_color_map[cluster] for cluster in obs_data['cluster']]\n",
    "        cluster_hex_colors = {key: mc.to_hex(value) for key, value in cluster_color_map.items()}  \n",
    "\n",
    "    else:\n",
    "        cluster_colors = None\n",
    "\n",
    "    # Flatten col_colors if it's nested\n",
    "    columns = []\n",
    "    legend_colors = {}\n",
    "    \n",
    "    if phenotype:\n",
    "        columns.append(phenotype_colors)\n",
    "        legend_colors.update(**phenotype_hex_colors)\n",
    "    if cluster:\n",
    "        columns.append(cluster_colors)\n",
    "        legend_colors.update(**cluster_hex_colors)\n",
    "    \n",
    "    # Plotting\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    # Adjust the size of the figure and the clustermap properties as needed\n",
    "    g = sns.clustermap(similarity_matrix, col_colors=columns, row_colors=columns, cmap=\"viridis\", yticklabels=False, col_cluster=c, row_cluster=c, figsize=(10, 10))\n",
    "    \n",
    "    # Add legend for phenotypes and clusters\n",
    "    legend_patches = [\n",
    "        mpatches.Patch(color=color, label=batch) for batch, color in legend_colors.items()\n",
    "    ]    \n",
    "    \n",
    "    plt.legend(handles=legend_patches, title=\"Metadata\", bbox_to_anchor=(1.06, 0.0), loc='upper right')\n",
    "    plt.savefig(f\"{workdir}/{name}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca05fdfc-15e7-4413-9280-cf55f003a262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity matrix for your data\n",
    "# Here, `data_matrix` should be the feature matrix for your samples\n",
    "\n",
    "heatmap_similarity(adata_all, layer=\"X_pca\", d=\"cosine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f55486-f956-481c-a73f-4da0f3867f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "workdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87448a0d-ebd9-4a4e-8bea-eef4204a6bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_similarity(adata_all, layer=\"X_pca\", d=\"cosine\", c=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7436b9-f3f2-4564-9804-0fab40ae3774",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_similarity(adata_all, layer=\"X_pca\", d=\"euclidean\")\n",
    "heatmap_similarity(adata_all, layer=\"X_pca\", d=\"euclidean\", c=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c46ae74-6b7d-406d-90b4-996a6fe72767",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_similarity(adata_all, layer=\"X\", d=\"euclidean\")\n",
    "heatmap_similarity(adata_all, layer=\"X\", d=\"euclidean\", c=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70b6b2-43db-4d6e-82fe-042fa667127f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_matrix = euclidean_distances(adata_all.X)\n",
    "adata_all.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10255be6-7602-4ba4-b519-935ceeecc37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract cluster assignments\n",
    "cluster_assignments = adata_all.obs[\"cluster\"]\n",
    "\n",
    "# Initialize containers for similarities\n",
    "within_cluster_stats = {}\n",
    "between_cluster_stats = {}\n",
    "\n",
    "# Initialize a matrix of zeros\n",
    "distance_matrix = np.zeros((len(clusters), len(clusters)))\n",
    "\n",
    "\n",
    "# Calculate within-cluster similarity\n",
    "for cluster in np.unique(cluster_assignments):\n",
    "    indices = np.where(cluster_assignments == cluster)[0]\n",
    "    within_distances = similarity_matrix[np.ix_(indices, indices)]\n",
    "    within_cluster_stats[cluster] = {\n",
    "        \"mean\": np.mean(within_distances),\n",
    "        \"std\": np.std(within_distances),\n",
    "        \"median\": np.median(within_distances),\n",
    "        \"distances\": within_distances\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate between-cluster similarity (simplified example)\n",
    "for cluster1, cluster2 in itertools.combinations(cluster_assignments, 2):\n",
    "        if cluster1 != cluster2 and (cluster2, cluster1) not in between_cluster_stats.keys():\n",
    "            indices1 = np.where(cluster_assignments == cluster1)[0]\n",
    "            indices2 = np.where(cluster_assignments == cluster2)[0]\n",
    "            between_distances = similarity_matrix[np.ix_(indices1, indices2)].flatten()\n",
    "            between_cluster_stats[(cluster1, cluster2)] = {\n",
    "                \"mean\": np.mean(between_distances),\n",
    "                \"std\": np.std(between_distances),\n",
    "                \"median\": np.median(between_distances),\n",
    "                \"distances\": between_distances\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49989d73-d608-4f4e-98d9-593571e96c99",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Extract unique cluster names\n",
    "clusters = set()\n",
    "for pair in between_cluster_stats.keys():\n",
    "    clusters.update(pair)\n",
    "clusters = sorted(list(clusters))\n",
    "\n",
    "# Initialize a matrix of zeros\n",
    "distance_matrix = np.zeros((len(clusters), len(clusters)))\n",
    "\n",
    "# Fill the matrix with your data\n",
    "for i, cluster1 in enumerate(clusters):\n",
    "    for j, cluster2 in enumerate(clusters):\n",
    "        if cluster1 == cluster2:\n",
    "            # Distance to self can be 0 or a max value for better visualization\n",
    "            distance_matrix[i, j] = within_cluster_stats.get(cluster1, np.nan)['mean']\n",
    "        else:\n",
    "            # Check both possible keys since the input might not have a consistent order\n",
    "            key = (cluster1, cluster2)\n",
    "            reverse_key = (cluster2, cluster1)\n",
    "            distance_matrix[i, j] = between_cluster_stats.get(key, between_cluster_stats.get(reverse_key, np.nan))['mean']\n",
    "\n",
    "# Convert the matrix into a DataFrame for better labeling in seaborn\n",
    "distance_df = pd.DataFrame(distance_matrix, index=clusters, columns=clusters)\n",
    "\n",
    "# Plot the heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(distance_df, annot=True, cmap=\"coolwarm\", fmt=\".4f\")\n",
    "plt.title(\"Cluster Distance Visualization\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{workdir}/Cluster_distance_euclidean.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948425d-c78a-4eb9-b6e6-c84a5c6d32bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a list to collect data\n",
    "boxplot_data = []\n",
    "\n",
    "# Iterate through each pair and their stats\n",
    "for (cluster1, cluster2), stats in between_cluster_stats.items():\n",
    "    for distance in stats['distances']:\n",
    "        # Append a tuple (or list) with the pair label and the distance\n",
    "        boxplot_data.append((f\"{cluster1} vs {cluster2}\", distance))\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "boxplot_df = pd.DataFrame(boxplot_data, columns=['Cluster Pair', 'Distance'])\n",
    "\n",
    "# Quick check on the DataFrame\n",
    "print(boxplot_df.head())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='Cluster Pair', y='Distance', data=boxplot_df, palette='pastel')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels for better readability\n",
    "plt.title('Between-Cluster Distances')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{workdir}/Cluster_distance_violin.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70758b60-e4fc-4faf-b667-f400ab5bd707",
   "metadata": {},
   "outputs": [],
   "source": [
    "wcluster = [\"Seurat_1\", \"Seurat_2\", \"chaotic_cnv\", \"normal_cnv_tripolar\"]\n",
    "# Create a boolean mask where each row is True if its 'cluster' value is in `wcluster`\n",
    "mask = adata_all.obs['cluster'].isin(wcluster)\n",
    "\n",
    "# Use the mask to select rows from `adata_all`\n",
    "adata_extracted = adata_all[mask, :]\n",
    "adata_extracted\n",
    "\n",
    "similarity_matrix = euclidean_distances(adata_extracted.X)\n",
    "\n",
    "# Extract cluster assignments\n",
    "cluster_assignments = adata_extracted.obs[\"cluster\"]\n",
    "\n",
    "# Initialize containers for similarities\n",
    "within_cluster_stats = {}\n",
    "between_cluster_stats = {}\n",
    "\n",
    "# Initialize a matrix of zeros\n",
    "distance_matrix = np.zeros((len(clusters), len(clusters)))\n",
    "\n",
    "\n",
    "# Calculate within-cluster similarity\n",
    "for cluster in np.unique(cluster_assignments):\n",
    "    indices = np.where(cluster_assignments == cluster)[0]\n",
    "    within_distances = similarity_matrix[np.ix_(indices, indices)]\n",
    "    within_cluster_stats[cluster] = {\n",
    "        \"mean\": np.mean(within_distances),\n",
    "        \"std\": np.std(within_distances),\n",
    "        \"median\": np.median(within_distances),\n",
    "        \"distances\": within_distances\n",
    "    }\n",
    "\n",
    "\n",
    "# Calculate between-cluster similarity (simplified example)\n",
    "for cluster1, cluster2 in itertools.combinations(cluster_assignments, 2):\n",
    "        if cluster1 != cluster2 and (cluster2, cluster1) not in between_cluster_stats.keys():\n",
    "            indices1 = np.where(cluster_assignments == cluster1)[0]\n",
    "            indices2 = np.where(cluster_assignments == cluster2)[0]\n",
    "            between_distances = similarity_matrix[np.ix_(indices1, indices2)].flatten()\n",
    "            between_cluster_stats[(cluster1, cluster2)] = {\n",
    "                \"mean\": np.mean(between_distances),\n",
    "                \"std\": np.std(between_distances),\n",
    "                \"median\": np.median(between_distances),\n",
    "                \"distances\": between_distances\n",
    "            }\n",
    "\n",
    "# Extract unique cluster names\n",
    "clusters = set()\n",
    "for pair in between_cluster_stats.keys():\n",
    "    clusters.update(pair)\n",
    "clusters = sorted(list(clusters))\n",
    "\n",
    "# Initialize a matrix of zeros\n",
    "distance_matrix = np.zeros((len(clusters), len(clusters)))\n",
    "\n",
    "# Fill the matrix with your data\n",
    "for i, cluster1 in enumerate(clusters):\n",
    "    for j, cluster2 in enumerate(clusters):\n",
    "        if cluster1 == cluster2:\n",
    "            # Distance to self can be 0 or a max value for better visualization\n",
    "            distance_matrix[i, j] = within_cluster_stats.get(cluster1, np.nan)['mean']\n",
    "        else:\n",
    "            # Check both possible keys since the input might not have a consistent order\n",
    "            key = (cluster1, cluster2)\n",
    "            reverse_key = (cluster2, cluster1)\n",
    "            distance_matrix[i, j] = between_cluster_stats.get(key, between_cluster_stats.get(reverse_key, np.nan))['mean']\n",
    "\n",
    "# Convert the matrix into a DataFrame for better labeling in seaborn\n",
    "distance_df = pd.DataFrame(distance_matrix, index=clusters, columns=clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1104da-f57b-4eea-b47a-493152717c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(distance_df, annot=True, cmap=\"coolwarm\", fmt=\".4f\")\n",
    "plt.title(\"Cluster Distance Visualization\")\n",
    "plt.savefig(f\"{workdir}/Cluster_subset_distance_euclidean.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff02abc3-9d4f-4ad2-ac0f-1cebc28cb77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxplot_data = []\n",
    "\n",
    "# Iterate through each pair and their stats\n",
    "for (cluster1, cluster2), stats in between_cluster_stats.items():\n",
    "    for distance in stats['distances']:\n",
    "        # Append a tuple (or list) with the pair label and the distance\n",
    "        boxplot_data.append((f\"{cluster1} vs {cluster2}\", distance))\n",
    "\n",
    "# Convert to DataFrame for easier plotting\n",
    "boxplot_df = pd.DataFrame(boxplot_data, columns=['Cluster Pair', 'Distance'])\n",
    "\n",
    "# Quick check on the DataFrame\n",
    "print(boxplot_df.head())\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='Cluster Pair', y='Distance', data=boxplot_df, palette='pastel')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate labels for better readability\n",
    "plt.title('Between-Cluster Distances')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{workdir}/Cluster_subset_distance_violin.pdf\", pad_inches=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec47d23-a22d-44ab-b265-0c91b7be9089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Step 2: Pairwise comparisons\n",
    "p_values = []\n",
    "comparisons = []\n",
    "\n",
    "# Sample data from the previous step, representing 'distances' for different cluster pairs\n",
    "sample_distances = {\n",
    "    ('Seurat_1', 'Seurat_2'): between_cluster_stats[('Seurat_1', 'Seurat_2')]['distances'],\n",
    "    ('Seurat_1', 'chaotic_cnv'): between_cluster_stats[('Seurat_1', 'chaotic_cnv')]['distances'],\n",
    "    ('Seurat_1', 'normal_cnv_tripolar'): between_cluster_stats[('Seurat_1', 'normal_cnv_tripolar')]['distances'],\n",
    "    ('Seurat_2', 'chaotic_cnv'): between_cluster_stats[('Seurat_2', 'chaotic_cnv')]['distances'],\n",
    "    ('Seurat_2', 'normal_cnv_tripolar'): between_cluster_stats[('Seurat_2', 'normal_cnv_tripolar')]['distances'],\n",
    "    ('chaotic_cnv', 'normal_cnv_tripolar'): between_cluster_stats[('chaotic_cnv', 'normal_cnv_tripolar')]['distances'],\n",
    "}\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "normality_test_results = {cluster_pair: stats.shapiro(distances) for cluster_pair, distances in sample_distances.items()}\n",
    "print(normality_test_results)\n",
    "\n",
    "# Extract all unique cluster combinations\n",
    "clusters = list(between_cluster_stats.keys())\n",
    "\n",
    "for i in range(len(clusters)):\n",
    "    for j in range(i+1, len(clusters)):\n",
    "        cluster_pair_1 = clusters[i]\n",
    "        cluster_pair_2 = clusters[j]\n",
    "        \n",
    "        distances_1 = between_cluster_stats[cluster_pair_1]['distances']\n",
    "        distances_2 = between_cluster_stats[cluster_pair_2]['distances']\n",
    "        \n",
    "        # Perform Mann-Whitney U test\n",
    "        stat, p = stats.mannwhitneyu(distances_1, distances_2, alternative='two-sided')\n",
    "        p_values.append(p)\n",
    "        comparisons.append(f\"{cluster_pair_1} vs {cluster_pair_2}\")\n",
    "\n",
    "# Step 3: Adjust for multiple comparisons (example using Benjamini-Hochberg)\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "rejections, corrected_p_values, _, _ = multipletests(p_values, alpha=0.05, method='fdr_bh')\n",
    "\n",
    "# Display results\n",
    "for comparison, p_value, reject in zip(comparisons, corrected_p_values, rejections):\n",
    "    print(f\"{comparison}: p={p_value:.4e}, significant={reject}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a3f511-6801-43fc-8642-8caadda91d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values, corrected_p_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc72f5b-9b91-406b-8082-3e12e4c5f96b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e677a6a-e678-4752-b698-f80c6c70607f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ce45a-a9e4-4fa0-a1a1-eaa2d7c20fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='Cluster Pair', y='Distance', data=boxplot_df, palette='pastel')\n",
    "plt.xticks(rotation=90)  # Rotate labels for better readability\n",
    "plt.title('Between-Cluster Distances')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b80af7d-9792-4487-a3f7-596648cbe369",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2716852-fa77-404d-b66f-408e3f263ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "within_cluster_stats\n",
    "pd.DataFrame.from_dict(within_cluster_stats, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f464c0-a691-4e45-8899-487725bdf1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(between_cluster_stats, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dafb686-cab0-423c-a797-6f6fa0b0b233",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "phenotype_count = {}\n",
    "\n",
    "ps = list(adata_all.obs['cluster'])\n",
    "\n",
    "for p in ps:\n",
    "    if p not in phenotype_count.keys():\n",
    "        phenotype_count[p] = 1\n",
    "    else:\n",
    "        phenotype_count[p] += 1\n",
    "\n",
    "pd.DataFrame.from_dict(phenotype_count, orient='index', columns=[\"Count\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856aa43-4161-45c1-80c6-b022c4d9c719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import session_info\n",
    "session_info.show(dependencies=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae0b306-1770-44e7-a62e-420b35e84421",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Heatmmap\"\n",
    "c = False\n",
    "d = 'euclidean'\n",
    "layer = 'X'\n",
    "name = f\"{name}_{'Clustered_' if c else ''}{d}.{layer}\"\n",
    "name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b128a171-3c2a-4c5e-ba0d-6b3e40e5ffed",
   "metadata": {},
   "source": [
    "## Debug code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d36ca3-e45a-42d0-8623-6ab03a7ff4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This is part of https://github.com/pinellolab/simba/blob/dev/simba/tools/_pbg.py\n",
    "To try and identify the error\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "id_ent = pd.Index([])  # ids of all entities\n",
    "dict_ent_type = dict()\n",
    "ctr_ent = 0  # counter for entity types\n",
    "entity_alias = pd.DataFrame(columns=['alias'])\n",
    "dict_graph_stats = dict()\n",
    "prefix = ''\n",
    "\n",
    "col_names = [\"source\", \"relation\", \"destination\", \"weight\"]\n",
    "\n",
    "df_edges = pd.DataFrame(columns=col_names)\n",
    "\n",
    "data_for_graph = list(all_datasets.values())\n",
    "\n",
    "# Adding a debug statement to check how many iterations in the loop\n",
    "print(f\"Number of elements in data_for_graph: {len(data_for_graph)}\")\n",
    "\n",
    "for ctr_rel, adata_ori in enumerate(data_for_graph):\n",
    "    print(f\"Processing data_for_graph element #{ctr_rel}\")\n",
    "\n",
    "    obs_names = adata_ori.obs_names\n",
    "    var_names = adata_ori.var_names\n",
    "\n",
    "    # Debug statement to check the current obs_names and var_names\n",
    "    print(f\"Current obs_names: {obs_names}\")\n",
    "    print(f\"Current var_names: {var_names}\")\n",
    "\n",
    "    if len(set(obs_names).intersection(id_ent)) == 0:\n",
    "        prefix_i = f'{prefix}{ctr_ent}'\n",
    "        id_ent = id_ent.union(adata_ori.obs_names)\n",
    "        entity_alias_obs = pd.DataFrame(\n",
    "            index=obs_names,\n",
    "            columns=['alias'],\n",
    "            data=[f'{prefix_i}.{x}'\n",
    "                  for x in range(len(obs_names))])\n",
    "\n",
    "        dict_ent_type[prefix_i] = obs_names\n",
    "        entity_alias = pd.concat(\n",
    "            [entity_alias, entity_alias_obs],\n",
    "            ignore_index=False)\n",
    "        obs_type = prefix_i\n",
    "        ctr_ent += 1\n",
    "\n",
    "        # Debug statement to confirm addition of new entity type\n",
    "        print(f\"Added new entity type: {prefix_i}\")\n",
    "\n",
    "    else:\n",
    "        for k, item in dict_ent_type.items():\n",
    "            if len(set(obs_names).intersection(item)) > 0:\n",
    "                obs_type = k\n",
    "                break\n",
    "        if not set(obs_names).issubset(id_ent):\n",
    "            id_ent = id_ent.union(adata_ori.obs_names)\n",
    "            adt_obs_names = list(set(obs_names)-set(item))\n",
    "            entity_alias_obs = pd.DataFrame(\n",
    "                index=adt_obs_names,\n",
    "                columns=['alias'],\n",
    "                data=[f'{prefix_i}.{len(item)+x}'\n",
    "                      for x in range(len(adt_obs_names))])\n",
    "            dict_ent_type[obs_type] = obs_names.union(adt_obs_names)\n",
    "            entity_alias = pd.concat(\n",
    "                [entity_alias, entity_alias_obs],\n",
    "                ignore_index=False)\n",
    "\n",
    "            # Debug statement for updated entity type\n",
    "            print(f\"Updated entity type: {obs_type}\")\n",
    "\n",
    "    if len(set(var_names).intersection(id_ent)) == 0:\n",
    "        prefix_i = f'{prefix}{ctr_ent}'\n",
    "        id_ent = id_ent.union(adata_ori.var_names)\n",
    "        entity_alias_var = pd.DataFrame(\n",
    "            index=var_names,\n",
    "            columns=['alias'],\n",
    "            data=[f'{prefix_i}.{x}'\n",
    "                  for x in range(len(var_names))])\n",
    "\n",
    "        dict_ent_type[prefix_i] = var_names\n",
    "        entity_alias = pd.concat(\n",
    "            [entity_alias, entity_alias_var],\n",
    "            ignore_index=False)\n",
    "        var_type = prefix_i\n",
    "        ctr_ent += 1\n",
    "\n",
    "        # Debug statement to confirm addition of new variable type\n",
    "        print(f\"Added new variable type: {prefix_i}\")\n",
    "\n",
    "    else:\n",
    "        for k, item in dict_ent_type.items():\n",
    "            if len(set(var_names).intersection(item)) > 0:\n",
    "                var_type = k\n",
    "                break\n",
    "        if not set(var_names).issubset(id_ent):\n",
    "            id_ent = id_ent.union(adata_ori.var_names)\n",
    "            adt_var_names = list(set(var_names)-set(item))\n",
    "            entity_alias_var = pd.DataFrame(\n",
    "                index=adt_var_names,\n",
    "                columns=['alias'],\n",
    "                data=[f'{prefix_i}.{len(item)+x}'\n",
    "                      for x in range(len(adt_var_names))])\n",
    "            dict_ent_type[var_type] = var_names.union(adt_var_names)\n",
    "            entity_alias = pd.concat(\n",
    "                [entity_alias, entity_alias_var],\n",
    "                ignore_index=False)\n",
    "\n",
    "            # Debug statement for updated variable type\n",
    "            print(f\"Updated variable type: {var_type}\")\n",
    "\n",
    "    arr_simba = adata_ori.X\n",
    "    _row, _col = arr_simba.nonzero()\n",
    "    df_edges_x = pd.DataFrame(columns=col_names)\n",
    "\n",
    "    # Issue is here! >>>>\n",
    "\n",
    "    print(_row, _col)\n",
    "    \n",
    "    print('Entity_alias:')\n",
    "    print(entity_alias)\n",
    "    print(\"OBS\")\n",
    "\n",
    "    print(entity_alias.loc[obs_names[_row], 'alias'])\n",
    "    print('VAR')\n",
    "\n",
    "    print(entity_alias.loc[var_names[_col], 'alias'])\n",
    "    \n",
    "    var_alias = entity_alias.loc[var_names[_col], 'alias'].values\n",
    "    df_edges_x['source'] = entity_alias.loc[obs_names[_row], 'alias'].values\n",
    "    df_edges_x['relation'] = f'r{ctr_rel}'\n",
    "    df_edges_x['destination'] = entity_alias.loc[var_names[_col], 'alias'].values\n",
    "    df_edges_x['weight'] = arr_simba[_row, _col].A.flatten()\n",
    "\n",
    "    # Debug statements for edges data frame\n",
    "    print(f\"Relation {ctr_rel}: source: {obs_type}, destination: {var_type}\")\n",
    "    print(f\"#edges: {df_edges_x.shape[0]}\")\n",
    "\n",
    "    df_edges = pd.concat([df_edges, df_edges_x], ignore_index=True)\n",
    "\n",
    "    # Debug statements for updated df_edges\n",
    "    print(f\"Updated df_edges after relation {ctr_rel}: {df_edges.shape}\")\n",
    "\n",
    "    adata_ori.obs['pbg_id'] = \"\"\n",
    "    adata_ori.var['pbg_id'] = \"\"\n",
    "    adata_ori.obs.loc[obs_names, 'pbg_id'] = entity_alias.loc[obs_names, 'alias'].copy()\n",
    "    adata_ori.var.loc[var_names, 'pbg_id'] = entity_alias.loc[var_names, 'alias'].copy()\n",
    "\n",
    "    # Debug statement after updating pbg_id\n",
    "    print(\"Updated pbg_id in adata_ori\")\n",
    "entity_alias\n",
    "\n",
    "entity_alias.index.isunique.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
